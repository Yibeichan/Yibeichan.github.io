<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="ZNp-Or9osyj9lyjTN9O5DRqaSId7rz-OEXbix-AvYjM" />










  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="coursera,machine learning," />





  <link rel="alternate" href="/atom.xml" title="舞！舞！舞！" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="笔记完成时间：0813(逻辑回归) 0806(Octave基本操作) 0805(多元线性回归) 0804(线代复习) 0803(一元线性回归) 0801(简介)0806：写个作业要看15页的文档，好难过。(做完作业觉得智商为负。转置总弄错！)0804：LaTex问题待解决。0803：怎么用markdown写数学公式？0801：抽风，笔记用英文开始，最后还是转向了中文。">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning 课程笔记 (未完)">
<meta property="og:url" content="http://yibeichen.me/2016/08/13/Machine-Learning/index.html">
<meta property="og:site_name" content="舞！舞！舞！">
<meta property="og:description" content="笔记完成时间：0813(逻辑回归) 0806(Octave基本操作) 0805(多元线性回归) 0804(线代复习) 0803(一元线性回归) 0801(简介)0806：写个作业要看15页的文档，好难过。(做完作业觉得智商为负。转置总弄错！)0804：LaTex问题待解决。0803：怎么用markdown写数学公式？0801：抽风，笔记用英文开始，最后还是转向了中文。">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_15:33:48.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_16:00:36.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_16:29:37.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_16:30:57.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_16:55:25.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_17:03:40.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_17:33:12.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_17:34:07.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_17:24:09.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_11:50:42.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_12:52:15.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_13:27:49.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_2016-08-03_14:59:51.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_15:00:09.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_19:21:13.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_19:25:25.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_19:28:36.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:06:28.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:06:55.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:22:28.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:39:01.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:41:31.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:51:46.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:54:22.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:58:42.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_18:39:42.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_CodeCogsEqn.png">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_18:57:29.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_19:38:23.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_19:48:35.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_19:57:46.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_20:02:28.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_20:24:45.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_20:28:58.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-04_20:38:21.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-05_21:15:45.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-05_21:24:06.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-05_21:30:28.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_21:33:11.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-05_22:03:22.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-05_22:12:59.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-06_11:58:45.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-06_12:01:59.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-06_12:05:59.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-06_12:12:17.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-06_CodeCogsEqn.png">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-06_CodeCogsEqn-1.png">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:20:24.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:28:46.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_CodeCogsEqn.png">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:32:26.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:52:19.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:59:47.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_18:06:59.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_18:07:58.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_20:29:27.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_20:38:27.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-08_20:47:42.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_2016-08-10_15:30:56.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_15:32:35.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_15:34:15.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_15:44:33.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_2016-08-10_15:45:00.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_15:52:25.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_2016-08-10_2016-08-10_16:00:27.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:06:06.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:12:31.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:12:31.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:29:58.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:34:40.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-12_15:59:41.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:14:01.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:18:03.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-12_2016-08-12_16:18:42.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:21:21.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:40:29.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:41:06.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:44:41.jpg">
<meta property="og:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-18_16:01:27.jpg">
<meta property="og:updated_time" content="2016-08-18T08:50:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning 课程笔记 (未完)">
<meta name="twitter:description" content="笔记完成时间：0813(逻辑回归) 0806(Octave基本操作) 0805(多元线性回归) 0804(线代复习) 0803(一元线性回归) 0801(简介)0806：写个作业要看15页的文档，好难过。(做完作业觉得智商为负。转置总弄错！)0804：LaTex问题待解决。0803：怎么用markdown写数学公式？0801：抽风，笔记用英文开始，最后还是转向了中文。">
<meta name="twitter:image" content="http://oapfne1kf.bkt.clouddn.com/2016-08-01_15:33:48.jpg">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yibeichen.me/2016/08/13/Machine-Learning/"/>

  <title> Machine Learning 课程笔记 (未完) | 舞！舞！舞！ </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">舞！舞！舞！</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">宁要壮烈的闪烁，不要平淡的沉默</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tools">
          <a href="/tools" rel="section">
            
            工具
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Machine Learning 课程笔记 (未完)
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-13T15:28:11+08:00" content="2016-08-13">
              2016-08-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/08/13/Machine-Learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/08/13/Machine-Learning/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>笔记完成时间：0813(逻辑回归) 0806(Octave基本操作) 0805(多元线性回归) 0804(线代复习) 0803(一元线性回归) 0801(简介)<br>0806：写个作业要看15页的文档，好难过。(做完作业觉得智商为负。转置总弄错！)<br>0804：LaTex问题待解决。<br>0803：怎么用markdown写数学公式？<br>0801：抽风，笔记用英文开始，最后还是转向了中文。<br><a id="more"></a></p>
<hr>
<p>Coursera地址: <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external">机器学习</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Machine Learning</p>
<ul>
<li>Grew out of work in AI</li>
<li>New capability for computers</li>
</ul>
<p>Tom Mitchell provides a more <strong>modern definition</strong>: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”<br>Example: playing checkers.<br>E = the experience of playing many games of checkers<br>T = the task of playing checkers.<br>P = the probability that the program will win the next game.</p>
<p>Examples:</p>
<ul>
<li>Database mining<ul>
<li>Web click data</li>
<li>medical records</li>
<li>biology</li>
<li>engineering</li>
</ul>
</li>
<li>Applications can’t program by hand<ul>
<li>Autonomous helicopter</li>
<li>handwriting recognition</li>
<li>most of Natural Language Processing(NLP)</li>
<li>Computer Vision</li>
</ul>
</li>
<li>Self-customizing programs<ul>
<li>Amazon product recommendations</li>
</ul>
</li>
<li>Understanding human learning brain(brain, real AI)</li>
</ul>
<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><p><em>probably the most common type of ML problem</em></p>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><h4 id="Housing-price-prediction"><a href="#Housing-price-prediction" class="headerlink" title="Housing price prediction"></a>Housing price prediction</h4><p>Given this data, you have a friend who owns a house that is 750 square feet and hoping to sell the house and they want to know how much the can get for the house.<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_15:33:48.jpg" alt="2016-08-01_15:33:48.jpg"></p>
<p><strong>How can the learning algorithm help you?</strong></p>
<ul>
<li>straight line through the data<br>maybe about $150,000</li>
<li>a second-order polynomial<br>closer to $200,000</li>
</ul>
<p>How to chose and how to decide do you want to fit a straight line to the data or do you want to fit the quadratic function to the data.(<strong>talk later</strong>)</p>
<p><strong>What does this mean?</strong><br>Supervised learning refers to the fact that we gave the algorithm a data set in which the “right answers” were given.</p>
<ul>
<li>we give the algorithm a data set of houses in which for every example in this data set, we told it what is the right price so what is the actual price that, that house sold for and the toss of the algorithm was to just produce more of these right answers.</li>
<li>also called <strong>a regression problem</strong> which means trying to predict a <em>continuous(not discrete)</em> value output.</li>
</ul>
<h4 id="Breast-cancer-malignant-benign"><a href="#Breast-cancer-malignant-benign" class="headerlink" title="Breast cancer (malignant, benign)"></a>Breast cancer (malignant, benign)</h4><p>Look at medical records and try to predict of a breast cancer as malignant or benign.<br>Let’s say a friend who tragically has a breast tumor, according to the size of her tumor, can you estimate what is the chance that a tumor is malignant versus benign?<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_16:00:36.jpg" alt="2016-08-01_16:00:36.jpg"></p>
<p><strong>a classification problem</strong></p>
<ul>
<li>refers to we’re trying to predict a <em>discrete</em> value output: zero or one, malignant or benign.</li>
<li>sometimes you can have more than two values for the two possible values for the output.<ul>
<li>e.g. there are three types of breast cancers and so you may try to predict the discrete value of zero, one, two, or three with zero being benign, one means type one cancer, two means a second type of cancers, and three means the third type.</li>
</ul>
</li>
</ul>
<p><strong>another way to plot the data</strong><br>map it down to a real line and use different symbols to denote malignant versus benign examples.<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_16:29:37.jpg" alt="2016-08-01_16:29:37.jpg"></p>
<p>↑↑↑we use only one feature or one attribute <em>tumor size</em><br>↓↓↓we have more than one feature, more than one attribute <em>age and tumor size</em><br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_16:30:57.jpg" alt="2016-08-01_16:30:57.jpg"><br>given a data set like this, the learning algorithm mighy throw the straight line through the data to try to separate out the malignant tumors from the benign ones.</p>
<p><strong>How do you deal with an infinite numbers of feature?</strong><br>an algorithm called the Support Vector Machine</p>
<ul>
<li>allow a computer to deal with an infinite numbers of features.</li>
</ul>
<h3 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h3><ul>
<li>Supervised learning lets you get the <em>correct</em> data</li>
<li>a regression problem–predict a continuous(not discrete) value output</li>
<li>a classification problem–predict a discrete value output</li>
</ul>
<h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><p>We’re given data that that doesn’t have any labels or that all has the same label or really no labels. And we’re not told what to do with it and what each data point is.<br>Instead we’re just told, here is a data set. Can you find some structure in the data?<br>an Unsupervised Learning algorithm might decide that the data lives in two different clusters<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_16:55:25.jpg" alt="2016-08-01_16:55:25.jpg"></p>
<h3 id="clustering-algorithms"><a href="#clustering-algorithms" class="headerlink" title="clustering algorithms"></a>clustering algorithms</h3><h4 id="Examples-1"><a href="#Examples-1" class="headerlink" title="Examples"></a>Examples</h4><h5 id="Google-URLs"><a href="#Google-URLs" class="headerlink" title="Google URLs"></a>Google URLs</h5><p>you click different links then you get different stories.<br>So what Google News has done is look for tens of thousands of news stories and automatically cluster them together. The news stories that are all about the same topic get displayed together.</p>
<h5 id="DNA-microarray-data"><a href="#DNA-microarray-data" class="headerlink" title="DNA microarray data"></a>DNA microarray data</h5><p> The idea is put a group of different individuals and for each of them, you measure how much they do or do not have a certain gene.<br> These colors, red, green, gray and so on, they show the degree to which different individuals do or do not have a specific gene.<br> <img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_17:03:40.jpg" alt="2016-08-01_17:03:40.jpg"><br> What you can do is run a clustering algorithm to group individuals into different categories or into different types of people.  </p>
<h4 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h4><ul>
<li>organize large computer clusters<br>at large data centers, figure out which machines tend to work together and if you put them together, you can make your data center work more efficiently</li>
<li>social network analysis<br>given knowledge about which friends you email the most or given your Facebook friends or your Google+ circles, can we automatically identify which are cohesive groups of friends, also which are groups of people that all know each other?</li>
<li>market segmentation<br>look at the customer data set and automatically discover market segments and group customers into different market segments so that you can automatically and more efficiently sell or market your different market segments together?</li>
<li>surprisingly astronomical data analysis<br>gives surprisingly interesting useful theories of how galaxies are born.</li>
</ul>
<h3 id="cocktail-party-problem-algorithm"><a href="#cocktail-party-problem-algorithm" class="headerlink" title="cocktail party problem algorithm"></a>cocktail party problem algorithm</h3><p>(好了，我要上中文了)<br>想象有一个鸡尾酒会，房间里到处都是人，同时交谈，声音会重叠，你可能连站在你对面的那个人说的话都听不见。<br>那么重新想象一个只有两个人1和2的鸡尾酒会，他们同时交谈，房间里有两个麦克风A和B，放在不同的位置，因为距离不同，所以两个麦克风对这两个人的录音效果也不同。可能有在麦克风A那里1的声音大2的声音小，在B那里则是1的声音小2的声音大。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_17:33:12.jpg" alt="2016-08-01_17:33:12.jpg"><br>但是A和B都记录了两种声音，output的时候就会感受到声音的重叠。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_17:34:07.jpg" alt="2016-08-01_17:34:07.jpg"><br>假如两个人同时用不同的语言法语和英语数数，那么output的混音就是什么都听不见了，所以我们需要把法语和英语分开输出。</p>
<p>这就是传说中的“鸡尾酒会问题算法”。<br>这么一个看上去简简单单的应用，区分两条音轨，用Java可能要写成千上万条代码。但是聪明机智锲而不舍的研究者们折腾出了下面这个，仅仅一条代码瞬间解决问题↓↓↓<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-01_17:24:09.jpg" alt="2016-08-01_17:24:09.jpg"><br>(看不懂这个公式)</p>
<h3 id="Octave"><a href="#Octave" class="headerlink" title="Octave"></a>Octave</h3><p><strong>[W,s,v] = svd((repmat(sum(x.<em>x,1), size(x,1),1).</em>x)*x’)</strong><br>用的是Octave，也就是我们这门课要用的编程环境。<br>Octave/Matlab 和 Java/C++ 高下立见是吧。(还有我的Python也被黑了)</p>
<ul>
<li>SVD(singular value decomposition)–linear algebra routine which is built into octave</li>
<li>using MATLAB to prototype is a really good way to do this</li>
</ul>
<h1 id="Linear-Regression-with-One-Variable-一元线性回归"><a href="#Linear-Regression-with-One-Variable-一元线性回归" class="headerlink" title="Linear Regression with One Variable (一元线性回归)"></a>Linear Regression with One Variable (一元线性回归)</h1><h2 id="Model-and-Cost-Function"><a href="#Model-and-Cost-Function" class="headerlink" title="Model and Cost Function"></a>Model and Cost Function</h2><h3 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h3><p>Housing Price Predict</p>
<ul>
<li>supervised learning algorithm</li>
<li>linear regression</li>
</ul>
<ol>
<li>Training set (this is your data set)</li>
<li>Notation (used throughout the course)<ul>
<li>m = number of training examples</li>
<li>x’s = input variables / features</li>
<li>y’s = output variable “target” variables<ul>
<li>(x,y) - single training example</li>
<li>(x^{i}, y^{i}) - specific example (i^{th} training example)<ul>
<li>i is an index to training set</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_11:50:42.jpg" alt="2016-08-03_11:50:42.jpg"><br>(把这一整张都截图下来不算侵权吧？忐忑)<br>这个函数的名字叫做hypothesis，跟科研中的“hypothesis”(假设)的含义有很大区别，反正约定俗称就这样称呼了，不用太惊讶。</p>
<p>在预测房价这个问题中，一个自变量x(房子的面积)，一个因变量y(房价)，被称为一元线性回归, univariate也是一元的意思。<br>那么，怎么来表示函数h(hypothesis的缩写)呢？<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_12:52:15.jpg" alt="2016-08-03_12:52:15.jpg"><br>就是一元一次函数啦：y=b+kx</p>
<h3 id="Cost-Function-成本函数"><a href="#Cost-Function-成本函数" class="headerlink" title="Cost Function (成本函数)"></a>Cost Function (成本函数)</h3><p>顾名思义，成本函数，当然是要降低成本，所以这个函数越小越好。</p>
<p>也就是，根据样本数据，拟合一条直线出来。通常，这些样本数据的分布并不是非常完美的，它们大致会落在一条直线的附近。这条直线具体长什么样，取决于h函数的参数。<br>参数选得好，h函数做预测就会比较精确。而成本函数就是用来评估参数的好坏。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_13:27:49.jpg" alt="2016-08-03_13:27:49.jpg"><br>有没有觉得跟标准差的公式很相似？<br>道理是一样的，标准差也是用来描述一组数据的波动状况呀，越小表示数据越稳定。<br>所以呢，这个函数的另一个名字叫做 squared error function。</p>
<p>hθ(x)是关于变量x的函数，J(θ1)是关于参数θ的函数。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_2016-08-03_14:59:51.jpg" alt="2016-08-03_2016-08-03_14:59:51.jpg"><br>对于每一个θ1，都有一个对应的J(θ1)曲线<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_15:00:09.jpg" alt="2016-08-03_15:00:09.jpg"><br>而我们的目标就是找到一个θ1，使得J(θ1)最小化，这张图上的最小值是θ1=1，我们就可以用这个θ1=1来设定我们的拟合函数h。</p>
<p>上面这个案例是简化了算法，将θ0设定为0，接下来就回到原始的复杂数据上，来感受一下更普遍一点的线性回归。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_19:21:13.jpg" alt="2016-08-03_19:21:13.jpg"><br>这下我们有了两个参数θ0,θ1，J(θ0,θ1)的图形就不再是一条线了，而是下面这种三维曲面图：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_19:25:25.jpg" alt="2016-08-03_19:25:25.jpg"><br>不过，为了便于理解，接下来还是用轮廓图讲解：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_19:28:36.jpg" alt="2016-08-03_19:28:36.jpg"><br>右边图中的椭圆，就是J(θ0,θ1)值相同的所有点的集合。<br>同心圆最中间那个圈的中心，就是函数J的最小值。</p>
<h2 id="Parameter-Learning"><a href="#Parameter-Learning" class="headerlink" title="Parameter Learning"></a>Parameter Learning</h2><h3 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent (梯度下降)"></a>Gradient Descent (梯度下降)</h3><p>这种算法在机器学习中应用得很广，不仅仅是一元线性回归，下面这个就是常用的做法：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:06:28.jpg" alt="2016-08-03_21:06:28.jpg"><br>假设有个函数J(不一定是线性回归)，要求它的成本函数，通常是要先设定θ0,θ1，把它们都初始为0，然后一点点改变这两个参数，使得成本函数的值最低，图形如下：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:06:55.jpg" alt="2016-08-03_21:06:55.jpg"><br>为了更好地理解梯度下降，你可以把图上的这两个高点想象成公园的两座小山，你站在其中一座山的某一点上，环顾四周，问自己：“假如要以最快的速度下山，我该走哪个方向呢？”你找到了一个方向，往前走了一步，现在你站在新的起点上，再一次问自己刚刚那个相同的问题。然后又迈出了一步……如此重复这几个步骤，直到你抵达局部最低点。<br>这个算法的有趣之处在于，如果你的起点偏离了一些，你得到的局部最优解也是不同的。</p>
<p>接下来是梯度下降算法的定义：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:22:28.jpg" alt="2016-08-03_21:22:28.jpg"><br>这张图特意保留了笔记，助于日后唤醒记忆。<br>α 指的是learning rate，它控制着我们更新参数θj的幅度。<br>α 旁边的那串是导数项(derivative term)<br>要注意的是赋值的时候要同步更新 <strong>simultaneous update</strong><br>做课间练习的时候我还错了！明明右边的算法是错的，我还是用了它……</p>
<p>接下来讲了偏导数，不懂，没关系，课程还是可以继续下去！<br>至于导数，现学：就是求曲线上一点的切线的斜率，如果斜率为正，那就是正导数。下面这两个图就是一个正导数和一个负导数：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:39:01.jpg" alt="2016-08-03_21:39:01.jpg"></p>
<p>接下来看看α的大小对梯度下降的影响：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:41:31.jpg" alt="2016-08-03_21:41:31.jpg"><br>如果它的值过大，θ1会跑得很快，可能永远也找不到最优解。<br>如果它的值过小，θ1跑得会慢一点，但肯定不会错过最低点。</p>
<p>假如我们初始化的θ1已经是最低点了，再用梯度下降这个公式会怎么样呢？<br>保持不变，因为导数为0了呀。<br>正是由于导数的存在，所以在越接近最低点的时候θ1的值变化得越小，因为斜率越小呀，所以不用总是改变α的值。</p>
<h3 id="Gradient-Descent-For-Linear-Regression"><a href="#Gradient-Descent-For-Linear-Regression" class="headerlink" title="Gradient Descent For Linear Regression"></a>Gradient Descent For Linear Regression</h3><p>先复习一下这两个算法，如何将梯度下降应用到线性回归的成本函数中呢？最重要的部分是导数那块：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:51:46.jpg" alt="2016-08-03_21:51:46.jpg"><br>来，让我们推导一下：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:54:22.jpg" alt="2016-08-03_21:54:22.jpg"><br>再代入一下：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-03_21:58:42.jpg" alt="2016-08-03_21:58:42.jpg"><br>还记得吗？梯度下降的图形有两个山头，所以它有好几个局部最优解，但是成本函数的图形是一个 <strong>凸函数</strong>，又称 <strong>弓形函数</strong>，只有一个全局最优解。所以呢，用梯度下降来计算成本函数，只会得到一个全局最优解。</p>
<p>两种算法结合在一起构成的新的算法，就是线性回归算法，又称为批量梯度下降 <strong>“Batch” Gradient Descent</strong>。<br>“批量”意味着，下降过程中用到了所有样本数据。<br>也有一些其他的梯度下降方法，并不会用到所有的数据，而只是用了这些数据的子集。以后的课程会介绍。</p>
<p>线性代数中有一种称为正规方程 <strong>normal equations</strong> 的方法，可以不用梯度下降，也能求出J的最小值。不过，梯度下降更适用于large data。</p>
<h1 id="Linear-Algebra-Review"><a href="#Linear-Algebra-Review" class="headerlink" title="Linear Algebra Review"></a>Linear Algebra Review</h1><h2 id="Matrices-and-Vectors-矩阵和向量"><a href="#Matrices-and-Vectors-矩阵和向量" class="headerlink" title="Matrices and Vectors (矩阵和向量)"></a>Matrices and Vectors (矩阵和向量)</h2><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><p>矩阵可以理解为一个二维数组，矩阵的维度计算：行数乘以列数，比如4X2矩阵(R4X2,注意数字是上标)。<br>如何来表示一个特定的矩阵元素呢？以下图为例：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_18:39:42.jpg" alt="2016-08-04_18:39:42.jpg"><br>所以，A11=1402, A12=191, A43=undefined (注意，数字是下标)</p>
<h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><p>向量是一个nX1的矩阵，n是行数，1是列数，向量的维度计算以行数为参考，比如4维向量(R4,数字是上标)。<br>向量内的元素引用公式为：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_CodeCogsEqn.png" alt="2016-08-04_CodeCogsEqn.png"><br>({y}^{i}={i}^{th}$)<br>通常有两种向量索引表示方法：1-索引和0-索引，如下图：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_18:57:29.jpg" alt="2016-08-04_18:57:29.jpg"><br>很像Python里的数组，用的是0-indexed。<br>在数学上，用1-indexed比较多，在机器学习中0-indexed则更广泛些。</p>
<p>最后，在矩阵和向量的使用过程中，通常会用大写字母来表示矩阵，用小写字母表示数字或向量。</p>
<h2 id="Addition-and-Scalar-Multiplication-加法和标量乘法"><a href="#Addition-and-Scalar-Multiplication-加法和标量乘法" class="headerlink" title="Addition and Scalar Multiplication (加法和标量乘法)"></a>Addition and Scalar Multiplication (加法和标量乘法)</h2><h3 id="矩阵加法"><a href="#矩阵加法" class="headerlink" title="矩阵加法"></a>矩阵加法</h3><p>两个矩阵相加，就是把两个矩阵里的元素逐个相加，只有两个维度相同的矩阵才可以相加。</p>
<h3 id="标量乘法"><a href="#标量乘法" class="headerlink" title="标量乘法"></a>标量乘法</h3><p>scalar指的是real number，也就是说，一个实数和一个矩阵相乘，就是把矩阵中的每个元素都和这个实数相乘。除法也是同理。乘除前后的矩阵维度是不变的。</p>
<h3 id="综合运算-combination-of-operands"><a href="#综合运算-combination-of-operands" class="headerlink" title="综合运算(combination of operands)"></a>综合运算(combination of operands)</h3><p>也很简单啦，和实数的综合运算规律一样：先乘除后加减。</p>
<h2 id="Matrix-Vector-Multiplication-矩阵向量乘法"><a href="#Matrix-Vector-Multiplication-矩阵向量乘法" class="headerlink" title="Matrix Vector Multiplication(矩阵向量乘法)"></a>Matrix Vector Multiplication(矩阵向量乘法)</h2><p>来看一个例子，一个3X2的矩阵和一个2X1向量相乘会得到一个3*1向量。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_19:38:23.jpg" alt="2016-08-04_19:38:23.jpg"><br>注意了，这里讲的是一个m x n矩阵和一个n x 1矩阵(n维向量)相乘，得到一个m维向量。<br>具体做法是：把矩阵A的第一行元素和向量x里对应的元素相乘后相加，得到向量y的第一个元素，再把矩阵第二行元素和向量x对应的元素相乘后相加，得到向量y的第二个元素……以此类推。<br>来看一下前几节课说的房价预测的案例，怎么用矩阵向量乘法来快速计算呢？如下图：<img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_19:48:35.jpg" alt="2016-08-04_19:48:35.jpg"><br>把它们弄成矩阵和向量，就可以很容易地用计算机语言比如Octave(Python也可以)来实现了：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:, <span class="number">1000</span>,</div><div class="line">  prediction(<span class="built_in">i</span>):……</div></pre></td></tr></table></figure></p>
<p>上面这行代码可能是Octave的，总之这样写代码很简洁高效。</p>
<h2 id="Matrix-Matrix-Multiplication-矩阵和矩阵相乘"><a href="#Matrix-Matrix-Multiplication-矩阵和矩阵相乘" class="headerlink" title="Matrix Matrix Multiplication (矩阵和矩阵相乘)"></a>Matrix Matrix Multiplication (矩阵和矩阵相乘)</h2><p>矩阵和矩阵相乘，可以把第二个矩阵拆分为多个向量，就变成了矩阵和向量相乘，最后把得到的向量组合成矩阵：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_19:57:46.jpg" alt="2016-08-04_19:57:46.jpg"><br>注意，相乘的两个矩阵是有特点的，前一个矩阵的列数等于后一个矩阵的行数。<br>再用房价预测的案例，当有多个假设函数的时候，怎么用矩阵和矩阵乘法来计算：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_20:02:28.jpg" alt="2016-08-04_20:02:28.jpg"><br>一次矩阵计算就可以得到12个预测结果，而且很多编程语言都有丰富的线性代数库，用起来非常方便。</p>
<h2 id="Matrix-Multiplication-Properties"><a href="#Matrix-Multiplication-Properties" class="headerlink" title="Matrix Multiplication Properties"></a>Matrix Multiplication Properties</h2><p>介绍一些关于矩阵乘法的特性：</p>
<ol>
<li>乘法交换律对矩阵并不适用。(《上帝掷骰子吗？》讲到了是薛定谔还是谁？就是通过矩阵的交换乘法发现了量子的特征？啊呀，不记得了。)</li>
<li>矩阵符合乘法结合律。</li>
</ol>
<h3 id="单位矩阵-identity-matrix"><a href="#单位矩阵-identity-matrix" class="headerlink" title="单位矩阵 (identity matrix)"></a>单位矩阵 (identity matrix)</h3><p>这样来理解吧，在实数乘法中，1乘以任何数都等于任何数，所以1就是一个identity，而在矩阵界也有这样的数：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_20:24:45.jpg" alt="2016-08-04_20:24:45.jpg"><br>看看这个销魂的板书，这种单位矩阵的特点就是，对角线都是1，其他部分全部是0.</p>
<h2 id="Inverse-and-Transpose-求倒和转置"><a href="#Inverse-and-Transpose-求倒和转置" class="headerlink" title="Inverse and Transpose (求倒和转置)"></a>Inverse and Transpose (求倒和转置)</h2><h3 id="求倒运算"><a href="#求倒运算" class="headerlink" title="求倒运算"></a>求倒运算</h3><p>实数界除了0以外的数都有倒数，那么矩阵界呢？<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_20:28:58.jpg" alt="2016-08-04_20:28:58.jpg"><br>注意，只有方阵才有导数，如果方阵的所有元素都是0，也是没有倒数的，矩阵和它的倒数相乘得到的是单位矩阵，所以其实是根据单位矩阵来推算矩阵倒数，很多编程软件都有求逆矩阵的软件库。</p>
<p>另外，对于那些没有倒数的矩阵，他们被称为奇异(singular)矩阵或者退化(degenerate)矩阵。</p>
<h3 id="转置运算"><a href="#转置运算" class="headerlink" title="转置运算"></a>转置运算</h3><p>最直观的理解就是，沿着矩阵的对角线画一条线，再以这条线为轴将矩阵翻转。来继续看一看销魂的板书吧：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-04_20:38:21.jpg" alt="2016-08-04_20:38:21.jpg"><br>如何检验转置运算的对错？看第一个和最后一个元素，它俩是不变的。</p>
<h1 id="Linear-Regression-with-Multiple-Variables"><a href="#Linear-Regression-with-Multiple-Variables" class="headerlink" title="Linear Regression with Multiple Variables"></a>Linear Regression with Multiple Variables</h1><h2 id="Multivariate-Linear-Regression-多元线性回归"><a href="#Multivariate-Linear-Regression-多元线性回归" class="headerlink" title="Multivariate Linear Regression (多元线性回归)"></a>Multivariate Linear Regression (多元线性回归)</h2><h3 id="Multiple-Features-多变量"><a href="#Multiple-Features-多变量" class="headerlink" title="Multiple Features (多变量)"></a>Multiple Features (多变量)</h3><p>上一个案例：房价预测，只有一个变量是x(房子的面积)。现在，变成多变量就是x1(房间面积)、x2(房子楼层)、x3(房子的年数)……<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-05_21:15:45.jpg" alt="2016-08-05_21:15:45.jpg"><br>这个图很好看懂吧？多个变量可以凑成一个向量。<br>为了帮助理解，上标表示第几行，下标表示第几列。<br>那么函数怎么表示呢？一个变量是一元一次函数，多个变量当然就是多元一次函数啦。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-05_21:24:06.jpg" alt="2016-08-05_21:24:06.jpg"><br>图上第一行就是这个多元一次函数，手写部分是简化这个函数的步骤，还记得上节课说的转置运算吗？这个函数最终被简化为矩阵θ的倒数和矩阵x的乘积。<br>注意，这里引进了x_0，并设定它等于1.</p>
<h3 id="Gradient-Descent-for-Multiple-Variables"><a href="#Gradient-Descent-for-Multiple-Variables" class="headerlink" title="Gradient Descent for Multiple Variables"></a>Gradient Descent for Multiple Variables</h3><p>这节自然就是要讲用梯度下降来解决多元线性回归问题了。<br>首先，我们来看一下从一元变成多元，成本函数发生了什么样的变化：<br>注意，多个参数和多个变量都被写成了向量的形式。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-05_21:30:28.jpg" alt="2016-08-05_21:30:28.jpg"><br>接下来再看看梯度下降的公式有什么变化：<br>仔细看一下会发现，左右两边其实是一样的，运算的规则没有什么区别。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_21:33:11.jpg" alt="2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_2016-08-05_21:33:11.jpg"></p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>来，我们讲一讲梯度下降在实例中的应用</p>
<h4 id="Feature-Scaling-特征缩放"><a href="#Feature-Scaling-特征缩放" class="headerlink" title="Feature Scaling (特征缩放)"></a>Feature Scaling (特征缩放)</h4><p>这到底是什么呢？<br>这么说吧，它的目标就是要确保多个不同的特征(变量)在一个相似的范围内。<br>比如房价预测问题中，有两个变量x1(房间面积0-2000㎡)、x2(房间数量1-5)，这两个变量的取值相差太大了，这样画出来的成本函数就是那种非常细长的椭圆，再用梯度下降来算的话，会特别特别得慢。<br>那特征缩放是怎么做呢？<br>把x1除以2000、x2除以5，使得这两个变量的范围都在[0，1]，这样的话，画出的成本函数图就比较圆了，梯度下降就会更快。</p>
<p>通常，我们把各个变量的范围通过特征缩放，控制在[-1,1]左右，上下有误差没关系，只要各个变量之间的差别不大就行。<br>这也是有公式可以用的，注意公式里出现的两个数，一个是平均值，一个是极差。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-05_22:03:22.jpg" alt="2016-08-05_22:03:22.jpg"></p>
<h4 id="Learning-Rate-学习率α"><a href="#Learning-Rate-学习率α" class="headerlink" title="Learning Rate (学习率α)"></a>Learning Rate (学习率α)</h4><p>还记得吧，α是梯度下降那个公式里的，所以这一小节主要讲两点：</p>
<ol>
<li>如何确保梯度下降是正确运行的，也就是debugging</li>
<li>如何选择α的值</li>
</ol>
<p>下面这张图提供了两种方法来检验梯度下降是否正确：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-05_22:12:59.jpg" alt="2016-08-05_22:12:59.jpg"></p>
<ol>
<li>看左边，以迭代的步数(不同的案例这个数是不同的)为横坐标，J(θ)的值为纵坐标，画出图形，正确的情况就是，每一次梯度下降都会使J(θ)减小，当图形无限接近x轴的时候，差不多就是求出最优解的时候了。如果图形是上升的曲线，差不多你的梯度下降就是错的了。</li>
<li>看右边，用的是公式，如果每次迭代J(θ)的值减少的量小于10^{-3}，那么差不多就是快求出解了，但是这个问题在于10^{-3}这个数很难界定。</li>
</ol>
<p>所以呢，当我们发现梯度下降不太对的时候，就可以考虑换一下α的值了。比如，上图的图形如果是上升的，α的值就应该调小一点。<br>同样，如果图形是”W”型的曲线，α的值也应该调小。<br>当然啦，α太小了就会变得很慢很慢，图形的坡度也会变小。</p>
<p>建议：挑选α的值可以隔3倍试一试。比如：<br>……, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ……</p>
<h3 id="Features-and-Polynomial-多项式-Regression"><a href="#Features-and-Polynomial-多项式-Regression" class="headerlink" title="Features and Polynomial (多项式) Regression"></a>Features and Polynomial (多项式) Regression</h3><p>首先，我们来讲怎么选择特征。<br>还是拿房价预测的例子吧，假如有两个变量，一个是房子临街的宽度，一个是房子的深度，但在设函数的时候，我们可以把变量定为宽度和深度的乘积，这样就变成了一个一元线性回归问题。</p>
<p>多项式指的是什么呢？其实就是多次啦。<br>假如有一个案例只有一个变量，但是你发现用一次函数或者二次函数都不能很好地拟合这个数据集，那么你可以用三次函数，这样你就有了x、x²、x³，然后你可以重新定义一个有三个变量的函数，这三个变量分别就是x_1=x、x_2=x²、x_3=x³.不过要注意的是，这样一来三个变量的取值范围可能差别会很大，在梯度下降的时候别忘了缩放。<br>除了多项式，你还可以用根号呀，用一个变量表示x，另一个变量表示x的平方根，不过同样也要注意取值范围。</p>
<p>讲了这么多，其实是想说明，碰到回归问题不是只有一元线性回归这一种做法，有很多解决问题的方案，注意变换看问题的角度。</p>
<h2 id="Computing-Parameters-Analytically"><a href="#Computing-Parameters-Analytically" class="headerlink" title="Computing Parameters Analytically"></a>Computing Parameters Analytically</h2><h3 id="Normal-Equation-正规方程"><a href="#Normal-Equation-正规方程" class="headerlink" title="Normal Equation (正规方程)"></a>Normal Equation (正规方程)</h3><p>在这之前，我们讲的都是用梯度下降求最优解，可能要迭代很多次才能收敛到一个最小值，而正规方程则提供了一种新方法，一步求得最优解。<br>先考虑一下实数情况下，J(θ)是关于θ的一元二次函数，怎么求最解呢？微积分的方法是求θ的导数，再把导数置0，就可以求出答案了。<br>那假如θ是一个向量呢，难道要把所有θ的导数都求出来置0吗？这样也可以，但是好麻烦。</p>
<p>那正规方程怎么弄呢？构造向量，转置运算！<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-06_11:58:45.jpg" alt="2016-08-06_11:58:45.jpg"><br>看见上图最后一行的公式了吧，是不是有熟悉的T出现了？下面的图解释了一下怎么转置：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-06_12:01:59.jpg" alt="2016-08-06_12:01:59.jpg"><br>我们再回过头看看公式，到底是什么意思呢？<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-06_12:05:59.jpg" alt="2016-08-06_12:05:59.jpg"><br>X的转置乘以X，再对乘积取导数，然后又乘以X的转置，再乘以y。<br>其实还是很好理解的对吧。</p>
<p>既然正规方程看上去简洁很多，那是不是就全部用它求解呢？我们来看一下梯度下降和正规方程的对比：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-06_12:12:17.jpg" alt="2016-08-06_12:12:17.jpg"><br>总结一下，当n大于1万的时候，我们用梯度下降，小于1万我们用正规方程。这说的是线性回归，当涉及到其他更复杂的回归问题的时候，正规方程可能没这么好用，还是梯度下降更方便。</p>
<h3 id="Normal-Equation-Noninvertibility-正规方程不可逆性"><a href="#Normal-Equation-Noninvertibility-正规方程不可逆性" class="headerlink" title="Normal Equation Noninvertibility (正规方程不可逆性)"></a>Normal Equation Noninvertibility (正规方程不可逆性)</h3><p><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-06_CodeCogsEqn.png" alt="2016-08-06_CodeCogsEqn.png"><br>不可逆的问题考虑的是，括号里的乘积得到的矩阵是不可逆的怎么办呢？这样的矩阵被称为奇异或退化矩阵(前面说求倒运算的时候也提到过)。<br>我们来看一下，什么情况下，X的转置和X的乘积是不可逆的：</p>
<ol>
<li>数据中有冗余的变量。比如，房价预测的问题中，一个变量是平方英尺为单位的面积，另一个则是平方米为单位的面积。</li>
<li>变量太多了。比如，变量比样本量还多，数据只有10个，变量却有100个。这时候就要删除一些变量了。这个问题还可以通过regularization的方法来解决，以后会讲到。</li>
</ol>
<p>所以，如果发现你得到一个不可逆的矩阵，那就回过头去检查上面两种情况，不过通常，不可逆矩阵出现的概率很小。Octave里有两个函数<code>pinv()</code>和<code>inv()</code>，可以用来求倒数。</p>
<h1 id="Octave-Tutorial"><a href="#Octave-Tutorial" class="headerlink" title="Octave Tutorial"></a>Octave Tutorial</h1><h2 id="Basic-Operations"><a href="#Basic-Operations" class="headerlink" title="Basic Operations"></a>Basic Operations</h2><p><code>%%</code>在Octave里表示注释<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="comment">%% Change Octave prompt  </span></div><div class="line">PS1(<span class="string">'&gt;&gt; '</span>);</div><div class="line"><span class="comment">%% Change working directory in windows example:</span></div><div class="line">cd <span class="string">'c:/path/to/desired/directory name'</span></div><div class="line"><span class="comment">%% Note that it uses normal slashes and does not use escape characters for the empty spaces.</span></div><div class="line"></div><div class="line"><span class="comment">%% elementary operations</span></div><div class="line"><span class="number">5</span>+<span class="number">6</span></div><div class="line"><span class="number">3</span><span class="number">-2</span></div><div class="line"><span class="number">5</span>*<span class="number">8</span></div><div class="line"><span class="number">1</span>/<span class="number">2</span></div><div class="line"><span class="number">2</span>^<span class="number">6</span></div><div class="line"><span class="number">1</span> == <span class="number">2</span>  <span class="comment">% false</span></div><div class="line"><span class="number">1</span> ~= <span class="number">2</span>  <span class="comment">% true.  note, not "!="</span></div><div class="line"><span class="number">1</span> &amp;&amp; <span class="number">0</span></div><div class="line"><span class="number">1</span> || <span class="number">0</span></div><div class="line">xor(<span class="number">1</span>,<span class="number">0</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">%% variable assignment</span></div><div class="line">a = <span class="number">3</span>; <span class="comment">% semicolon suppresses output</span></div><div class="line">b = <span class="string">'hi'</span>;</div><div class="line">c = <span class="number">3</span>&gt;=<span class="number">1</span>;</div><div class="line"></div><div class="line"><span class="comment">% Displaying them:</span></div><div class="line">a = <span class="built_in">pi</span></div><div class="line"><span class="built_in">disp</span>(a)</div><div class="line"><span class="built_in">disp</span>(sprintf(<span class="string">'2 decimals: %0.2f'</span>, a))</div><div class="line"><span class="built_in">disp</span>(sprintf(<span class="string">'6 decimals: %0.6f'</span>, a))</div><div class="line">format long</div><div class="line">a</div><div class="line">format short</div><div class="line">a</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">%%  vectors and matrices</span></div><div class="line">A = [<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</div><div class="line"></div><div class="line">v = [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</div><div class="line">v = [<span class="number">1</span>; <span class="number">2</span>; <span class="number">3</span>]</div><div class="line">v = [<span class="number">1</span>:<span class="number">0.1</span>:<span class="number">2</span>]  <span class="comment">% from 1 to 2, with stepsize of 0.1. Useful for plot axes</span></div><div class="line">v = <span class="number">1</span>:<span class="number">6</span>        <span class="comment">% from 1 to 6, assumes stepsize of 1 (row vector)</span></div><div class="line"></div><div class="line">C = <span class="number">2</span>*<span class="built_in">ones</span>(<span class="number">2</span>,<span class="number">3</span>)  <span class="comment">% same as C = [2 2 2; 2 2 2]</span></div><div class="line">w = <span class="built_in">ones</span>(<span class="number">1</span>,<span class="number">3</span>)    <span class="comment">% 1x3 vector of ones</span></div><div class="line">w = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">3</span>)</div><div class="line">w = <span class="built_in">rand</span>(<span class="number">1</span>,<span class="number">3</span>)  <span class="comment">% drawn from a uniform distribution</span></div><div class="line">w = <span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">3</span>) <span class="comment">% drawn from a normal distribution (mean=0, var=1)</span></div><div class="line">w = <span class="number">-6</span> + <span class="built_in">sqrt</span>(<span class="number">10</span>)*(<span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">10000</span>));  <span class="comment">% (mean = -6, var = 10) - note: add the semicolon</span></div><div class="line">hist(w)     <span class="comment">% plot histogram using 10 bins (default)</span></div><div class="line">hist(w,<span class="number">50</span>)  <span class="comment">% plot histogram using 50 bins</span></div><div class="line"><span class="comment">% note: if hist() crashes, try "graphics_toolkit('gnu_plot')"</span></div><div class="line"></div><div class="line">I = <span class="built_in">eye</span>(<span class="number">4</span>)    <span class="comment">% 4x4 identity matrix</span></div><div class="line"></div><div class="line"><span class="comment">% help function</span></div><div class="line">help <span class="built_in">eye</span></div><div class="line">help <span class="built_in">rand</span></div><div class="line">help help</div></pre></td></tr></table></figure></p>
<h2 id="Moving-Data-Around"><a href="#Moving-Data-Around" class="headerlink" title="Moving Data Around"></a>Moving Data Around</h2><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">%% dimensions</div><div class="line">sz = size(A) % <span class="number">1</span>x2 matrix: [(<span class="built_in">number</span> <span class="keyword">of</span> rows) (<span class="built_in">number</span> <span class="keyword">of</span> columns)]</div><div class="line">size(A,<span class="number">1</span>)  % <span class="built_in">number</span> <span class="keyword">of</span> rows</div><div class="line">size(A,<span class="number">2</span>)  % <span class="built_in">number</span> <span class="keyword">of</span> cols</div><div class="line"><span class="built_in">length</span>(v)  % size <span class="keyword">of</span> longest dimension</div><div class="line"></div><div class="line"></div><div class="line">%% loading data</div><div class="line">pwd    % show current <span class="built_in">directory</span> (current path)</div><div class="line">cd <span class="string">'C:\Users\ang\Octave files'</span>   % change <span class="built_in">directory</span></div><div class="line">ls     % list <span class="built_in">files</span> <span class="keyword">in</span> current <span class="built_in">directory</span></div><div class="line"><span class="built_in">load</span> q1y.dat    % alternatively, <span class="built_in">load</span>(<span class="string">'q1y.dat'</span>)</div><div class="line"><span class="built_in">load</span> q1x.dat</div><div class="line">who    % list variables <span class="keyword">in</span> workspace</div><div class="line">whos   % list variables <span class="keyword">in</span> workspace (<span class="keyword">detailed</span> view)</div><div class="line"><span class="built_in">clear</span> q1y       % <span class="built_in">clear</span> <span class="keyword">command</span> <span class="title">without</span> <span class="title">any</span> <span class="title">args</span> <span class="title">clears</span> <span class="title">all</span> <span class="title">vars</span></div><div class="line">v = q1x(<span class="number">1</span>:<span class="number">10</span>);  % <span class="keyword">first</span> <span class="number">10</span> elements <span class="keyword">of</span> q1x (counts down <span class="keyword">the</span> columns)</div><div class="line">save hello.mat v;   % save <span class="built_in">variable</span> v <span class="keyword">into</span> <span class="built_in">file</span> hello.mat</div><div class="line">save hello.txt v -ascii; % save <span class="keyword">as</span> ascii</div><div class="line">% fopen, fread, fprintf, fscanf also work  [[<span class="keyword">not</span> needed <span class="keyword">in</span> class]]</div><div class="line"></div><div class="line">%% indexing</div><div class="line">A(<span class="number">3</span>,<span class="number">2</span>)  % indexing is (row,col)</div><div class="line">A(<span class="number">2</span>,:)  % <span class="built_in">get</span> <span class="keyword">the</span> <span class="number">2</span>nd row.</div><div class="line">        % <span class="string">":"</span> means every <span class="keyword">element</span> along that dimension</div><div class="line">A(:,<span class="number">2</span>)  % <span class="built_in">get</span> <span class="keyword">the</span> <span class="number">2</span>nd col</div><div class="line">A([<span class="number">1</span> <span class="number">3</span>],:) % print all  <span class="keyword">the</span> elements <span class="keyword">of</span> rows <span class="number">1</span> <span class="keyword">and</span> <span class="number">3</span></div><div class="line"></div><div class="line">A(:,<span class="number">2</span>) = [<span class="number">10</span>; <span class="number">11</span>; <span class="number">12</span>]     % change <span class="keyword">second</span> column</div><div class="line">A = [A, [<span class="number">100</span>; <span class="number">101</span>; <span class="number">102</span>]]; % append column vec</div><div class="line">A(:) % Select all elements <span class="keyword">as</span> <span class="keyword">a</span> column vector.</div><div class="line"></div><div class="line">% Putting data together</div><div class="line">A = [<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</div><div class="line">B = [<span class="number">11</span> <span class="number">12</span>; <span class="number">13</span> <span class="number">14</span>; <span class="number">15</span> <span class="number">16</span>] % same dims <span class="keyword">as</span> A</div><div class="line">C = [A B]  % concatenating A <span class="keyword">and</span> B matrices side <span class="keyword">by</span> side</div><div class="line">C = [A, B] % concatenating A <span class="keyword">and</span> B matrices side <span class="keyword">by</span> side</div><div class="line">C = [A; B] % Concatenating A <span class="keyword">and</span> B top <span class="keyword">and</span> bottom</div></pre></td></tr></table></figure>
<h2 id="Computing-on-Data"><a href="#Computing-on-Data" class="headerlink" title="Computing on Data"></a>Computing on Data</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">%%</span> initialize variables</div><div class="line">A = [<span class="number">1</span> <span class="number">2</span>;<span class="number">3</span> <span class="number">4</span>;<span class="number">5</span> <span class="number">6</span>]</div><div class="line">B = [<span class="number">11</span> <span class="number">12</span>;<span class="number">13</span> <span class="number">14</span>;<span class="number">15</span> <span class="number">16</span>]</div><div class="line">C = [<span class="number">1</span> <span class="number">1</span>;<span class="number">2</span> <span class="number">2</span>]</div><div class="line">v = [<span class="number">1</span>;<span class="number">2</span>;<span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="symbol">%%</span> <span class="built_in">matrix</span> operations</div><div class="line">A * C  <span class="symbol">%</span> <span class="built_in">matrix</span> multiplication</div><div class="line">A .* B <span class="symbol">%</span> element-wise multiplication</div><div class="line"><span class="symbol">%</span> A .* C  <span class="keyword">or</span> A * B gives <span class="built_in">error</span> - wrong <span class="built_in">dimensions</span></div><div class="line">A .^ <span class="number">2</span> <span class="symbol">%</span> element-wise square of each element <span class="keyword">in</span> A</div><div class="line"><span class="number">1</span>./v   <span class="symbol">%</span> element-wise reciprocal</div><div class="line"><span class="built_in">log</span>(v)  <span class="symbol">%</span> <span class="built_in">functions</span> like this operate element-wise on vecs <span class="keyword">or</span> matrices</div><div class="line"><span class="built_in">exp</span>(v)</div><div class="line"><span class="built_in">abs</span>(v)</div><div class="line"></div><div class="line">-v  <span class="symbol">%</span> -<span class="number">1</span>*v</div><div class="line"></div><div class="line">v + ones(<span class="built_in">length</span>(v), <span class="number">1</span>)  </div><div class="line"><span class="symbol">%</span> v + <span class="number">1</span>  <span class="symbol">%</span> same</div><div class="line"></div><div class="line">A'  <span class="symbol">%</span> <span class="built_in">matrix</span> <span class="built_in">transpose</span></div><div class="line"></div><div class="line"><span class="symbol">%%</span> misc useful <span class="built_in">functions</span></div><div class="line"></div><div class="line"><span class="symbol">%</span> <span class="built_in">max</span>  (<span class="keyword">or</span> <span class="built_in">min</span>)</div><div class="line">a = [<span class="number">1</span> <span class="number">15</span> <span class="number">2</span> <span class="number">0.5</span>]</div><div class="line">val = <span class="built_in">max</span>(a)</div><div class="line">[val,<span class="literal">ind</span>] = <span class="built_in">max</span>(a) <span class="symbol">%</span> val -  maximum element of the <span class="built_in">vector</span> a <span class="keyword">and</span> index - index value where maximum occur</div><div class="line">val = <span class="built_in">max</span>(A) <span class="symbol">%</span> <span class="keyword">if</span> A <span class="built_in">is</span> <span class="built_in">matrix</span>, returns <span class="built_in">max</span> from each column</div><div class="line"></div><div class="line"><span class="symbol">%</span> <span class="built_in">compare</span> <span class="built_in">values</span> <span class="keyword">in</span> a <span class="built_in">matrix</span> &amp; find</div><div class="line">a &lt; <span class="number">3</span> <span class="symbol">%</span> checks which <span class="built_in">values</span> <span class="keyword">in</span> a are less than <span class="number">3</span></div><div class="line">find(a &lt; <span class="number">3</span>) <span class="symbol">%</span> gives location of elements less than <span class="number">3</span></div><div class="line">A = magic(<span class="number">3</span>) <span class="symbol">%</span> generates a magic <span class="built_in">matrix</span> - <span class="keyword">not</span> much used <span class="keyword">in</span> ML algorithms</div><div class="line">[r,c] = find(A&gt;=<span class="number">7</span>)  <span class="symbol">%</span> <span class="built_in">row</span>, column <span class="built_in">indices</span> <span class="keyword">for</span> <span class="built_in">values</span> matching comparison</div><div class="line"></div><div class="line"><span class="symbol">%</span> <span class="built_in">sum</span>, prod</div><div class="line"><span class="built_in">sum</span>(a)</div><div class="line">prod(a)</div><div class="line"><span class="built_in">floor</span>(a) <span class="symbol">%</span> <span class="keyword">or</span> ceil(a)</div><div class="line"><span class="built_in">max</span>(rand(<span class="number">3</span>),rand(<span class="number">3</span>))</div><div class="line"><span class="built_in">max</span>(A,[],<span class="number">1</span>) -  maximum along <span class="built_in">columns</span>(defaults to <span class="built_in">columns</span> - <span class="built_in">max</span>(A,[]))</div><div class="line"><span class="built_in">max</span>(A,[],<span class="number">2</span>) - maximum along rows</div><div class="line">A = magic(<span class="number">9</span>)</div><div class="line"><span class="built_in">sum</span>(A,<span class="number">1</span>)</div><div class="line"><span class="built_in">sum</span>(A,<span class="number">2</span>)</div><div class="line"><span class="built_in">sum</span>(<span class="built_in">sum</span>( A .* eye(<span class="number">9</span>) ))</div><div class="line"><span class="built_in">sum</span>(<span class="built_in">sum</span>( A .* flipud(eye(<span class="number">9</span>)) ))</div><div class="line"></div><div class="line"></div><div class="line"><span class="symbol">%</span> Matrix inverse (pseudo-inverse)</div><div class="line">pinv(A)        <span class="symbol">%</span> inv(A'*A)*A'</div></pre></td></tr></table></figure>
<h2 id="Plotting-Data"><a href="#Plotting-Data" class="headerlink" title="Plotting Data"></a>Plotting Data</h2><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">%% plotting</div><div class="line">t = [<span class="number">0</span>:<span class="number">0</span>.<span class="number">01</span>:<span class="number">0</span>.<span class="number">98</span>]<span class="comment">;</span></div><div class="line">y1 = sin(<span class="number">2</span>*pi*<span class="number">4</span>*t)<span class="comment">;</span></div><div class="line">plot(t,y1)<span class="comment">;</span></div><div class="line">y2 = cos(<span class="number">2</span>*pi*<span class="number">4</span>*t)<span class="comment">;</span></div><div class="line">hold on<span class="comment">;  % "hold off" to turn off</span></div><div class="line">plot(t,y2,<span class="string">'r'</span>)<span class="comment">;</span></div><div class="line">xlabel(<span class="string">'time'</span>)<span class="comment">;</span></div><div class="line">ylabel(<span class="string">'value'</span>)<span class="comment">;</span></div><div class="line">legend(<span class="string">'sin'</span>,<span class="string">'cos'</span>)<span class="comment">;</span></div><div class="line">title(<span class="string">'my plot'</span>)<span class="comment">;</span></div><div class="line">print -dpng <span class="string">'myPlot.png'</span></div><div class="line"><span class="keyword">close; </span>          % <span class="keyword">or, </span> <span class="string">"close all"</span> to <span class="keyword">close </span>all figs</div><div class="line">figure(<span class="number">1</span>)<span class="comment">; plot(t, y1);</span></div><div class="line">figure(<span class="number">2</span>)<span class="comment">; plot(t, y2);</span></div><div class="line">figure(<span class="number">2</span>), clf<span class="comment">;  % can specify the figure number</span></div><div class="line"><span class="keyword">subplot(1,2,1); </span> % <span class="keyword">Divide </span>plot into <span class="number">1</span>x2 grid, access <span class="number">1</span>st element</div><div class="line">plot(t,y1)<span class="comment">;</span></div><div class="line"><span class="keyword">subplot(1,2,2); </span> % <span class="keyword">Divide </span>plot into <span class="number">1</span>x2 grid, access <span class="number">2</span>nd element</div><div class="line">plot(t,y2)<span class="comment">;</span></div><div class="line">axis([<span class="number">0</span>.<span class="number">5</span> <span class="number">1</span> -<span class="number">1</span> <span class="number">1</span>])<span class="comment">;  % change axis scale</span></div><div class="line"></div><div class="line">%% <span class="keyword">display </span>a matrix (<span class="keyword">or </span>image)</div><div class="line">figure<span class="comment">;</span></div><div class="line">imagesc(magic(<span class="number">15</span>)), colorbar, colormap gray<span class="comment">;</span></div><div class="line">% comma-chaining function calls.  </div><div class="line">a=<span class="number">1</span>,<span class="keyword">b=2,c=3</span></div><div class="line">a=<span class="number">1</span><span class="comment">;b=2;c=3;</span></div></pre></td></tr></table></figure>
<h2 id="Control-Statements-for-while-if-statement"><a href="#Control-Statements-for-while-if-statement" class="headerlink" title="Control Statements: for, while, if statement"></a>Control Statements: for, while, if statement</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">v = <span class="built_in">zeros</span>(<span class="number">10</span>,<span class="number">1</span>);</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">10</span>,</div><div class="line">    v(<span class="built_in">i</span>) = <span class="number">2</span>^<span class="built_in">i</span>;</div><div class="line"><span class="keyword">end</span>;</div><div class="line"><span class="comment">% Can also use "break" and "continue" inside for and while loops to control execution.</span></div><div class="line"></div><div class="line"><span class="built_in">i</span> = <span class="number">1</span>;</div><div class="line"><span class="keyword">while</span> <span class="built_in">i</span> &lt;= <span class="number">5</span>,</div><div class="line">  v(<span class="built_in">i</span>) = <span class="number">100</span>;</div><div class="line">  <span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="built_in">i</span> = <span class="number">1</span>;</div><div class="line"><span class="keyword">while</span> true,</div><div class="line">  v(<span class="built_in">i</span>) = <span class="number">999</span>;</div><div class="line">  <span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span>;</div><div class="line">  <span class="keyword">if</span> <span class="built_in">i</span> == <span class="number">6</span>,</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">end</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> v(<span class="number">1</span>)==<span class="number">1</span>,</div><div class="line">  <span class="built_in">disp</span>(<span class="string">'The value is one!'</span>);</div><div class="line"><span class="keyword">elseif</span> v(<span class="number">1</span>)==<span class="number">2</span>,</div><div class="line">  <span class="built_in">disp</span>(<span class="string">'The value is two!'</span>);</div><div class="line"><span class="keyword">else</span></div><div class="line">  <span class="built_in">disp</span>(<span class="string">'The value is not one or two!'</span>);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<h2 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h2><p>首先，你在写字板上写了一个函数，保存格式为“functionName.m”<br>比如：<br><figure class="highlight delphi"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span> = <span class="title">squareThisNumber</span><span class="params">(x)</span></span></div><div class="line"><span class="title">y</span> = <span class="title">x</span>^2;</div></pre></td></tr></table></figure></p>
<p>然后，让Octave运行这个函数，有两种做法：</p>
<ol>
<li><p>用cd设置函数的位置，这样Octave才能找到它</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">% Navigate <span class="built_in">to</span> <span class="built_in">directory</span>:</div><div class="line">   cd /path/<span class="built_in">to</span>/<span class="function"><span class="keyword">function</span></span></div><div class="line"></div><div class="line">   % Call <span class="keyword">the</span> <span class="function"><span class="keyword">function</span>:</span></div><div class="line">   functionName(args)</div></pre></td></tr></table></figure>
</li>
<li><p>用路径搜索的命令，这个属于高级一点的，不会用也没关系</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">% To <span class="built_in">add</span> <span class="keyword">the</span> path <span class="keyword">for</span> <span class="keyword">the</span> current session <span class="keyword">of</span> Octave:</div><div class="line">    addpath(<span class="string">'/path/to/function/'</span>)</div><div class="line">% To remember <span class="keyword">the</span> path <span class="keyword">for</span> future sessions <span class="keyword">of</span> Octave, <span class="keyword">after</span> executing addpath above, also <span class="built_in">do</span>:</div><div class="line">    savepath</div></pre></td></tr></table></figure>
</li>
</ol>
<p>Octave有个优势在于，可以定义一个函数，返回多个值或参数<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[y1, y2]</span> = <span class="title">squareandCubeThisNo</span><span class="params">(x)</span></span></div><div class="line">    y1 = x^<span class="number">2</span></div><div class="line">    y2 = x^<span class="number">3</span></div></pre></td></tr></table></figure></p>
<p>上面这个函数的调用方式如下：<br><code>[a,b] = squareandCubeThisNo(x)</code></p>
<h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>向量化是一种很方便的运算方式，代码写得更少，出错也少，举个例子<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-06_CodeCogsEqn-1.png" alt="2016-08-06_CodeCogsEqn-1.png"><br>这个求和函数，正常的做法可能是用if语句写循环，类似这种：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">prediction = <span class="number">0.0</span>;</div><div class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:n+<span class="number">1</span>,</div><div class="line">  prediction += theta(<span class="built_in">j</span>) * x(<span class="built_in">j</span>);</div><div class="line"><span class="keyword">end</span>;</div></pre></td></tr></table></figure></p>
<p>但是，把它向量化，设定两个向量θ和x，函数就可以表示为θ的转置乘以x，用代码写出来就是：<br><code>prediction = theta&#39; * x;</code></p>
<h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><p>主要是考察了成本函数和梯度下降，其实有两种做法：</p>
<ol>
<li>用向量解</li>
<li>不用向量，普通解法</li>
</ol>
<p>用向量的优势在于，一句代码就可以，但涉及向量的转置，以及向量乘法不符合交换律，所以我做错了很多遍，现在还是糊涂的。<br>先看computeCost这个<br>正确答案是<br><code>J = 1/(2*m)*sum((X*theta - y).^2);</code><br>不好意思，这个没用向量，不过我用向量写的是<br><code>J = 1/(2*m)*(theta&#39;*X - y)^2);</code><br>首先，我的问题在于平方前没加“.”，然后转置运算错了，至今没有弄清楚到底应该转谁！<br>听说这样写也行：<br><code>J = 1/(2*m)*sum((theta&#39;*X&#39;-y&#39;).^2);</code><br>这里转置了三次呀！为我的智商捉急。</p>
<p>再看gradientDescent这个<br>正确答案1：<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">gradJ = X'*(<span class="name">X*theta</span> - y)/m<span class="comment">;</span></div><div class="line">theta = theta - alpha*gradJ<span class="comment">;</span></div></pre></td></tr></table></figure></p>
<p>正确答案2：<br><figure class="highlight gcode"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">temp<span class="number">1</span> = theta<span class="comment">(1)</span> - <span class="comment">(alpha / m)</span> * sum<span class="comment">((X * theta - y)</span>.* X<span class="comment">(:,1)</span>);  </div><div class="line">temp<span class="number">2</span> = theta<span class="comment">(2)</span> - <span class="comment">(alpha / m)</span> * sum<span class="comment">((X * theta - y)</span>.* X<span class="comment">(:,2)</span>);  </div><div class="line">theta<span class="comment">(1)</span> = temp<span class="number">1</span>;  </div><div class="line">theta<span class="comment">(2)</span> = temp<span class="number">2</span>;</div></pre></td></tr></table></figure></p>
<p>答案1用了向量，答案2则是普通解法。<br>很不幸，我还是转置错了，等等，为什么这个向量就没用sum求和呢？刚才cost怎么就得用sum呢！</p>
<p>另外，还有好几道选做的多元线性回归题目，没做，可以等全学完了再回头做做玩玩。</p>
<h1 id="Logistic-Regression-逻辑回归"><a href="#Logistic-Regression-逻辑回归" class="headerlink" title="Logistic Regression (逻辑回归)"></a>Logistic Regression (逻辑回归)</h1><h2 id="Classification-and-Representation"><a href="#Classification-and-Representation" class="headerlink" title="Classification and Representation"></a>Classification and Representation</h2><h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><p>比如：是不是垃圾邮件、肿瘤是良性还是恶性的……<br>这类问题中，我们想要预测的y，有两个值{0,1}，其中0：“Negative Class”(负类)，1：“Positive Class”(正类)<br>之后我们会讨论多分类问题，y有多个值{0，1，2，3……}，不过目前我们还是讨论二元分类问题。<br>先来说一下为什么线性回归不适合这种分类问题，看下图：<br>样本拟合的直线很可能会因为一个额外的样本量而发生改变，比如从红色线移到蓝色线，直线改变了，拟合结果也就不准确了。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:20:24.jpg" alt="2016-08-08_17:20:24.jpg"><br>还有一点是，通常分类问题中y = 0 or 1，但用线性回归可能会出现h<em>θ(x)&gt;1 or &lt;0<br>所以呢，逻辑回归出场了，它的取值就可以保证0&lt;=h</em>θ(x)&lt;=1</p>
<h3 id="Hypothesis-Representation"><a href="#Hypothesis-Representation" class="headerlink" title="Hypothesis Representation"></a>Hypothesis Representation</h3><p>逻辑回归中的函数长这样：<br>看右边的图，两边无限接近于0到1，因为图像长得像S，所以又被称为Sigmid函数，跟Logistic函数是一个意思啦，可以换着用。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:28:46.jpg" alt="2016-08-08_17:28:46.jpg"><br>公式如下：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_CodeCogsEqn.png" alt="2016-08-08_CodeCogsEqn.png"><br>其实这个函数的意思呢，是表示一个概率，比如病人的肿瘤是恶性(y=1)的可能性有多少？带进去算一算，差不多0.7吧(注意，那他的肿瘤是良性的概率就是1-0。7)。这句话写成数学公式就可以表示为：h_θ(x)= P(y=1|x,θ)，后面这就是个概率公式啊！看图↓<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:32:26.jpg" alt="2016-08-08_17:32:26.jpg"></p>
<h3 id="Decision-Boundary-决策边界"><a href="#Decision-Boundary-决策边界" class="headerlink" title="Decision Boundary (决策边界)"></a>Decision Boundary (决策边界)</h3><p>刚刚我们在例子里用公式，其实预测的是y=1的概率，那么到底应该什么时候预测y=1什么时候预测y=0呢？(总觉得这俩都差不多，一定是我没搞懂！)图中给了一个解决方案，当h_θ(x)&gt;=0.5就预测y=1的，反之则预测y=0.<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:52:19.jpg" alt="2016-08-08_17:52:19.jpg"><br>根据上面这个公式及理论，来看看下面这个例子：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_17:59:47.jpg" alt="2016-08-08_17:59:47.jpg"><br>看到坐标轴上的线了吗，那个就是决策边界，是根据参数和假设函数算出来的。这个图看上去好像那条线是根据样本数据的分布画出来的，事实上，把样本数据去掉也是一样的，它是算出来的，不是看到数据在图上的分布画的。(感觉我解释得有点艰难……)<br>再来看一个更复杂一点的案例，这次决策边界不是线性的了。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_18:06:59.jpg" alt="2016-08-08_18:06:59.jpg"><br>看到了吧，由于参数的选择，最后得到的是一个圆，所以就可以预测圆里面的是y=0，圆外面的是y=1.<br>还有更复杂的，图形画出来就更诡异了：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_18:07:58.jpg" alt="2016-08-08_18:07:58.jpg"><br>上面这两个案例都是多项式，前面我们讲过的。</p>
<h2 id="Logistic-Regression-Model"><a href="#Logistic-Regression-Model" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h2><h3 id="成本函数"><a href="#成本函数" class="headerlink" title="成本函数"></a>成本函数</h3><p><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_20:29:27.jpg" alt="2016-08-08_20:29:27.jpg"><br>看到了吗，图上用了一个函数嵌套，当我们考察里面这个函数cost的时候，看它的图形，就可以判断可不可以用梯度下降，当图形是凸形的时候，先降后升，才能找到一个全局最优解。<br>上面说的是线性回归的成本函数，来看看逻辑回归的，我们需要分类讨论下：</p>
<ol>
<li>当y=1的时候，函数图像如下(这个是根据对数函数的图像来的)，这个图形的意思是：当h<em>θ(x)=1，y=1，所以成本为0啊，预测准确了呀，但是当h</em>θ(x)=0的时候y还等于1，那就是你跟病人说你的肿瘤是良性的没问题，但实际上人家就是恶性的，你的预测错了，所以成本就非常之大，趋向无穷大！<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_20:38:27.jpg" alt="2016-08-08_20:38:27.jpg"></li>
<li>当y=0的时候，图像如下，跟上面的道理是一样的<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-08_20:47:42.jpg" alt="2016-08-08_20:47:42.jpg"><h3 id="简化成本函数和梯度下降"><a href="#简化成本函数和梯度下降" class="headerlink" title="简化成本函数和梯度下降"></a>简化成本函数和梯度下降</h3>首先，我们来看一下成本函数推导：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_2016-08-10_15:30:56.jpg" alt="2016-08-10_2016-08-10_15:30:56.jpg"><br>再来看一下正式的公式：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_15:32:35.jpg" alt="2016-08-10_15:32:35.jpg"><br>接下来看看梯度下降：<br>仔细观察这张图就会发现逻辑回归和线性回归的梯度下降公式似乎是一样的，的确长一样，不过这里的h_θ(x)不一样了。同样推荐向量化和特征缩放。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_15:34:15.jpg" alt="2016-08-10_15:34:15.jpg"><h3 id="Advanced-Optimization-高级优化"><a href="#Advanced-Optimization-高级优化" class="headerlink" title="Advanced Optimization (高级优化)"></a>Advanced Optimization (高级优化)</h3>通常呢，要优化J_(θ)(图上写的那两个)的值，我们需要写一些代码来不断更新参数的值，比如在梯度下降中是这样的：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_15:44:33.jpg" alt="2016-08-10_15:44:33.jpg"><br>但除了梯度下降之外，还有一些其他更高级的优化方法，它们的有点就是不用更新α的值，但是它们相对也复杂很多，不过，使用这个算法并不用完全理解它们的内在运作方式。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_2016-08-10_15:45:00.jpg" alt="2016-08-10_2016-08-10_15:45:00.jpg"><br>来看一下怎么用：<br>右边的function开始那几段是写在.m文件里给Octave调用的，下面的options那几段是要输入在Octave里的。要注意的是，有一句初始化θ的命令，θ必须是d维的向量，d大于等于2.<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_15:52:25.jpg" alt="2016-08-10_15:52:25.jpg"><br>现在说一下怎么应用到逻辑回归问题中去，就是代码怎么写的问题啦，注意一下gradient的下标<h2 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h2>这一节是讲怎么将逻辑回归应用到多分类问题中，涉及到“一对多”(one-vs-all)的分类算法。<br>先来看三个例子：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_2016-08-10_2016-08-10_16:00:27.jpg" alt="2016-08-10_2016-08-10_2016-08-10_16:00:27.jpg"><br>下图是二元问题和多分类问题training set的图示：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:06:06.jpg" alt="2016-08-10_16:06:06.jpg"><br>二元问题可以画一条线，那三元问题怎么办呢？分成三个二元问题：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:12:31.jpg" alt="2016-08-10_16:12:31.jpg"><br>这样我们就得到三个分类器了，那对于出现一个新的x，到底用哪个分类器预测呢？<br>把x代入到这些分类器中，选择概率最大的那个。(很好，我听得稀里糊的)<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:12:31.jpg" alt="2016-08-10_16:12:31.jpg"><h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><h2 id="Solving-the-Problem-of-Overfitting"><a href="#Solving-the-Problem-of-Overfitting" class="headerlink" title="Solving the Problem of Overfitting"></a>Solving the Problem of Overfitting</h2><h3 id="The-Problem-of-Overfitting-过度拟合问题"><a href="#The-Problem-of-Overfitting-过度拟合问题" class="headerlink" title="The Problem of Overfitting (过度拟合问题)"></a>The Problem of Overfitting (过度拟合问题)</h3>先看线性回归问题中的过度拟合<br>下图第三个图就是过度拟合的例子，现有的多项式似乎很好地拟合了所有training set中的数据，但它波动太大了，很难确定它是不是能正确预测training set外的数据。而图二就刚好，图一则是欠拟合。所以，过度拟合一般发生在特征过多的情况下……就是下面那一大段英文啦。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:29:58.jpg" alt="2016-08-10_16:29:58.jpg"><br>逻辑回归中的过度拟合道理也是相似的<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-10_16:34:40.jpg" alt="2016-08-10_16:34:40.jpg"><br>那么过度拟合的问题该怎么解决呢？两个办法</li>
</ol>
<ul>
<li>减少特征的量，因为这是导致过度拟合的原因啊，但是删除某些特征也就意味着失去了某些信息</li>
<li>用正则化的方法(也就是这一节要讲的方法)，不减少特征，但使每一项的参数值变小，也就减小了每个特征的影响，但保证了其完整性。</li>
</ul>
<h3 id="成本函数-1"><a href="#成本函数-1" class="headerlink" title="成本函数"></a>成本函数</h3><p>这里说的是正则化的成本函数，它有两个目标：</p>
<ul>
<li>充分拟合training set</li>
<li>降低参数θ的值，使拟合函数趋于简单</li>
</ul>
<p><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-12_15:59:41.jpg" alt="2016-08-12_15:59:41.jpg"><br>上面的成本函数，比线性回归的多了一项，所以，要求得最小值，意味着两项的值都要很小，从而降低了θ的值。而λ 是正则化参数(regularization parameter)。<br>如果λ非常大，会发生什么呢？<br>所有的θ都会被惩罚，变得很小，那就不能很好地拟合训练集了，变成了underfitting</p>
<h3 id="Regularized-Linear-Regression-正则化线性回归"><a href="#Regularized-Linear-Regression-正则化线性回归" class="headerlink" title="Regularized Linear Regression (正则化线性回归)"></a>Regularized Linear Regression (正则化线性回归)</h3><p>再来看看正则化的梯度下降。<br>这张图中，把θ_0分离开了，因为在成本函数中，我们要惩罚的是θ_j，而j是从1开始的。<br>所以从θ_1开始算起，在原来梯度下降的公式上又加了一项，如下图：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:14:01.jpg" alt="2016-08-12_16:14:01.jpg"><br>公式整理如下：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:18:03.jpg" alt="2016-08-12_16:18:03.jpg"><br>其中，下面这项始终是＜1的，这就一步一步降低了θ_j的值。<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-12_2016-08-12_16:18:42.jpg" alt="2016-08-12_2016-08-12_16:18:42.jpg"><br>接下来把梯度下降的公式向量化：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:21:21.jpg" alt="2016-08-12_16:21:21.jpg"><br>λ后面的是一个(n+1)(n+1)维的矩阵，长得很像单位矩阵，但是它的第一个元素是0不是1.</p>
<h3 id="Regularized-Logistic-Regression-正则化逻辑回归"><a href="#Regularized-Logistic-Regression-正则化逻辑回归" class="headerlink" title="Regularized Logistic Regression (正则化逻辑回归)"></a>Regularized Logistic Regression (正则化逻辑回归)</h3><p>来复习一下，逻辑回归的成本函数：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:40:29.jpg" alt="2016-08-12_16:40:29.jpg"><br>把它正则化，则是要加上下面这一项：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:41:06.jpg" alt="2016-08-12_16:41:06.jpg"><br>再来看一下，代码里是怎么做的：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-12_16:44:41.jpg" alt="2016-08-12_16:44:41.jpg"><br>用的还是<code>fminunc</code></p>
<h2 id="作业-1"><a href="#作业-1" class="headerlink" title="作业"></a>作业</h2><p>声明一下，并不是独立完成的，参考了许多位大神的代码，致谢！</p>
<h3 id="先说ex2"><a href="#先说ex2" class="headerlink" title="先说ex2"></a>先说ex2</h3><p>第一步是画图，不会用plotData，于是用了<code>help plot</code>，勉强写了几句，但是不成功，折腾了半天意识到：</p>
<ul>
<li>Octave不在乎空格，但是在乎全角半角，一切标点都要用英文的</li>
<li>只要用了<code>;</code>，换不换行都不重要</li>
</ul>
<p>最后的答案是：<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">pos=find(<span class="name">y==1</span>)<span class="comment">;</span></div><div class="line">neg=find(<span class="name">y==0</span>)<span class="comment">;</span></div><div class="line">plot(<span class="name">X</span>(<span class="name">pos</span>,<span class="number">1</span>),X(<span class="name">pos</span>,<span class="number">2</span>),'k+')<span class="comment">;</span></div><div class="line">plot(<span class="name">X</span>(<span class="name">neg</span>,<span class="number">1</span>),X(<span class="name">neg</span>,<span class="number">2</span>),'ko','MarkerFaceColor', 'm')<span class="comment">;</span></div></pre></td></tr></table></figure></p>
<p>给圆点加了颜色，不然不好看，plot还有很多其他属性property-value，比如：<br>“linestyle”,”linewidth”, “color”, “marker”, “markersize”, “markeredgecolor”,”markerfacecolor”</p>
<p>尴尬，折腾半天回去看pdf才发现，这一段已经写好了……<br><figure class="highlight gams"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">% Find Indices of <span class="keyword">Positive</span> <span class="keyword">and</span> <span class="keyword">Negative</span> Examples</div><div class="line">pos = find(y==<span class="number">1</span>); neg = find(y == <span class="number">0</span>);</div><div class="line">% Plot Examples</div><div class="line"><span class="function"><span class="title">plot</span>(<span class="params">X(pos, 1</span>)</span>, X(pos, <span class="number">2</span>), <span class="string">'k+'</span>,<span class="string">'LineWidth'</span>, <span class="number">2</span>, ...</div><div class="line">     <span class="string">'MarkerSize'</span>, <span class="number">7</span>);</div><div class="line"><span class="function"><span class="title">plot</span>(<span class="params">X(neg, 1</span>)</span>, X(neg, <span class="number">2</span>), <span class="string">'ko'</span>, <span class="string">'MarkerFaceColor'</span>, <span class="string">'y'</span>, ...</div><div class="line">     <span class="string">'MarkerSize'</span>, <span class="number">7</span>);</div></pre></td></tr></table></figure></p>
<p>还是直接贴代码+解释吧<br>sigmoid.m里该填的代码：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g = (<span class="number">1</span> + e.^(<span class="number">-1</span> *z)).^(<span class="number">-1</span>);</div></pre></td></tr></table></figure></p>
<p>这句就是把数学公式改成程序语言。<br>costFunction.m里该填的代码：<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">h=sigmoid(<span class="name">X*theta</span>)<span class="comment">;</span></div><div class="line">J=1/m*(<span class="name">-1*y</span>'*log(<span class="name">h</span>)-(<span class="number">1</span>-y')*log(1-h));</div><div class="line">grad=1/m*(<span class="name">h-y</span>)'*X<span class="comment">;</span></div></pre></td></tr></table></figure></p>
<p>这是用程序语言把三个函数分别表示出来，拟合函数、成本函数、梯度下降<br>predict.m里该填的代码：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">h=sigmoid(X*theta);</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:m</div><div class="line">    <span class="keyword">if</span> h(<span class="built_in">i</span>)&gt;=<span class="number">0.5</span></div><div class="line">       p(<span class="built_in">i</span>)=<span class="number">1</span>;</div><div class="line">    <span class="keyword">else</span></div><div class="line">       p(<span class="built_in">i</span>)=<span class="number">0</span>;</div><div class="line">    <span class="keyword">end</span>;</div><div class="line"><span class="keyword">end</span>;</div></pre></td></tr></table></figure></p>
<p>对着代码解释就是很轻松啊，然而当初想的时候，差点被折腾死，<code>i</code>是我随便用的一个，估计用其他字母也都可以。<code>1：m</code>这句有点像Python里面的list的用法。</p>
<h3 id="接下来是ex2-reg"><a href="#接下来是ex2-reg" class="headerlink" title="接下来是ex2_reg"></a>接下来是ex2_reg</h3><p>这个涉及到了正则化，只用补充一个costFunctionReg.m的代码：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">h=sigmoid(X*theta);</div><div class="line">J=<span class="number">1</span>/m*(-y'*<span class="built_in">log</span>(h)-(<span class="number">1</span>-y')*<span class="built_in">log</span>(<span class="number">1</span>-h))+lambda/(<span class="number">2</span>*m)*(theta(<span class="number">2</span>:<span class="keyword">end</span>)'*theta(<span class="number">2</span>:<span class="keyword">end</span>));</div><div class="line">grad=<span class="number">1</span>/m*[(h-y)<span class="string">'*X]+lambda/m*theta'</span>;</div><div class="line">grad(<span class="number">1</span>)=grad(<span class="number">1</span>)-lambda/m*theta(<span class="number">1</span>);</div></pre></td></tr></table></figure></p>
<p>就是这个，折腾了我半天，总提示我syntax error。看到<code>theta(2:end)&#39;*theta(2:end)</code>了吗，本来我写的是平方，但是显然向量只能写成两个相乘的方式，平方不行。<br>另外，先定义grad，再定义grad(1)，后面等于前面的减去一项。<br>转置的问题还是不太清楚，为什么<code>grad=1/m*[(h-y)&#39;*X]+lambda/m*theta&#39;;</code>这里的theta要转置！<br>PS：整整一周啊！陷入对智商的重大怀疑中，其实老师讲的已经很易懂了……</p>
<h1 id="Neural-Networks-Representation"><a href="#Neural-Networks-Representation" class="headerlink" title="Neural Networks: Representation"></a>Neural Networks: Representation</h1><h2 id="Motivations"><a href="#Motivations" class="headerlink" title="Motivations"></a>Motivations</h2><h3 id="Non-linear-Hypotheses"><a href="#Non-linear-Hypotheses" class="headerlink" title="Non-linear Hypotheses"></a>Non-linear Hypotheses</h3><p>为什么有了线性回归和逻辑回归算法还要学神经网络算法？<br>主要适用于非线性问题，为了准确描述此类问题通常需要用到多项式，如果特征很多，多项式会变得非常非常长。<br>比如说计算机识别图像这个案例。和人眼不同，计算机看到的图像是一堆像素，所以这个识别图像问题到计算机这就变成了：给你一组汽车的图片作为training set，训练出一个分类器，再给你一张照片，判断它是不是汽车。只不过，这里的x是像素……超级多的像素。<br>感受一下下面这张图：<br><img src="http://oapfne1kf.bkt.clouddn.com/2016-08-18_16:01:27.jpg" alt="2016-08-18_16:01:27.jpg"></p>
<h3 id="Neurons-and-the-Brain"><a href="#Neurons-and-the-Brain" class="headerlink" title="Neurons and the Brain"></a>Neurons and the Brain</h3><p>神经网络算法源于人们想拥有一种模仿大脑的算法。<br>视频里举了很多例子，比如传感器、声纳等等。</p>
<h2 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h2><h3 id="Model-Representation-I"><a href="#Model-Representation-I" class="headerlink" title="Model Representation I"></a>Model Representation I</h3>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/coursera/" rel="tag">#coursera</a>
          
            <a href="/tags/machine-learning/" rel="tag">#machine learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/08/09/Introduction-to-Classical-Music/" rel="next" title="Introduction to Classical Music 课程笔记 (未完)">
                <i class="fa fa-chevron-left"></i> Introduction to Classical Music 课程笔记 (未完)
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/08/13/HTML-CSS-Javascript-for-Web-Developers/" rel="prev" title="HTML, CSS, and Javascript for Web Developers 课程笔记 (未完)">
                HTML, CSS, and Javascript for Web Developers 课程笔记 (未完) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Yibei Chen" />
          <p class="site-author-name" itemprop="name">Yibei Chen</p>
          <p class="site-description motion-element" itemprop="description">Keep writing and catch that python</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">31</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Yibeichan" target="_blank" title="github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/adelechen" target="_blank" title="zhihu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  zhihu
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://chatnone.com/" title="ChatNone" target="_blank">ChatNone</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Supervised-Learning"><span class="nav-number">1.1.</span> <span class="nav-text">Supervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Examples"><span class="nav-number">1.1.1.</span> <span class="nav-text">Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Housing-price-prediction"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Housing price prediction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Breast-cancer-malignant-benign"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">Breast cancer (malignant, benign)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Recap"><span class="nav-number">1.1.2.</span> <span class="nav-text">Recap</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsupervised-Learning"><span class="nav-number">1.2.</span> <span class="nav-text">Unsupervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#clustering-algorithms"><span class="nav-number">1.2.1.</span> <span class="nav-text">clustering algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Examples-1"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Google-URLs"><span class="nav-number">1.2.1.1.1.</span> <span class="nav-text">Google URLs</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DNA-microarray-data"><span class="nav-number">1.2.1.1.2.</span> <span class="nav-text">DNA microarray data</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Applications"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Applications</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cocktail-party-problem-algorithm"><span class="nav-number">1.2.2.</span> <span class="nav-text">cocktail party problem algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Octave"><span class="nav-number">1.2.3.</span> <span class="nav-text">Octave</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-Regression-with-One-Variable-一元线性回归"><span class="nav-number">2.</span> <span class="nav-text">Linear Regression with One Variable (一元线性回归)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-and-Cost-Function"><span class="nav-number">2.1.</span> <span class="nav-text">Model and Cost Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Representation"><span class="nav-number">2.1.1.</span> <span class="nav-text">Model Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost-Function-成本函数"><span class="nav-number">2.1.2.</span> <span class="nav-text">Cost Function (成本函数)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parameter-Learning"><span class="nav-number">2.2.</span> <span class="nav-text">Parameter Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent-梯度下降"><span class="nav-number">2.2.1.</span> <span class="nav-text">Gradient Descent (梯度下降)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent-For-Linear-Regression"><span class="nav-number">2.2.2.</span> <span class="nav-text">Gradient Descent For Linear Regression</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-Algebra-Review"><span class="nav-number">3.</span> <span class="nav-text">Linear Algebra Review</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrices-and-Vectors-矩阵和向量"><span class="nav-number">3.1.</span> <span class="nav-text">Matrices and Vectors (矩阵和向量)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵"><span class="nav-number">3.1.1.</span> <span class="nav-text">矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#向量"><span class="nav-number">3.1.2.</span> <span class="nav-text">向量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Addition-and-Scalar-Multiplication-加法和标量乘法"><span class="nav-number">3.2.</span> <span class="nav-text">Addition and Scalar Multiplication (加法和标量乘法)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵加法"><span class="nav-number">3.2.1.</span> <span class="nav-text">矩阵加法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#标量乘法"><span class="nav-number">3.2.2.</span> <span class="nav-text">标量乘法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#综合运算-combination-of-operands"><span class="nav-number">3.2.3.</span> <span class="nav-text">综合运算(combination of operands)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix-Vector-Multiplication-矩阵向量乘法"><span class="nav-number">3.3.</span> <span class="nav-text">Matrix Vector Multiplication(矩阵向量乘法)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix-Matrix-Multiplication-矩阵和矩阵相乘"><span class="nav-number">3.4.</span> <span class="nav-text">Matrix Matrix Multiplication (矩阵和矩阵相乘)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix-Multiplication-Properties"><span class="nav-number">3.5.</span> <span class="nav-text">Matrix Multiplication Properties</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#单位矩阵-identity-matrix"><span class="nav-number">3.5.1.</span> <span class="nav-text">单位矩阵 (identity matrix)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inverse-and-Transpose-求倒和转置"><span class="nav-number">3.6.</span> <span class="nav-text">Inverse and Transpose (求倒和转置)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#求倒运算"><span class="nav-number">3.6.1.</span> <span class="nav-text">求倒运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#转置运算"><span class="nav-number">3.6.2.</span> <span class="nav-text">转置运算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-Regression-with-Multiple-Variables"><span class="nav-number">4.</span> <span class="nav-text">Linear Regression with Multiple Variables</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multivariate-Linear-Regression-多元线性回归"><span class="nav-number">4.1.</span> <span class="nav-text">Multivariate Linear Regression (多元线性回归)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiple-Features-多变量"><span class="nav-number">4.1.1.</span> <span class="nav-text">Multiple Features (多变量)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent-for-Multiple-Variables"><span class="nav-number">4.1.2.</span> <span class="nav-text">Gradient Descent for Multiple Variables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#应用"><span class="nav-number">4.1.3.</span> <span class="nav-text">应用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-Scaling-特征缩放"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">Feature Scaling (特征缩放)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-Rate-学习率α"><span class="nav-number">4.1.3.2.</span> <span class="nav-text">Learning Rate (学习率α)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Features-and-Polynomial-多项式-Regression"><span class="nav-number">4.1.4.</span> <span class="nav-text">Features and Polynomial (多项式) Regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Computing-Parameters-Analytically"><span class="nav-number">4.2.</span> <span class="nav-text">Computing Parameters Analytically</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal-Equation-正规方程"><span class="nav-number">4.2.1.</span> <span class="nav-text">Normal Equation (正规方程)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal-Equation-Noninvertibility-正规方程不可逆性"><span class="nav-number">4.2.2.</span> <span class="nav-text">Normal Equation Noninvertibility (正规方程不可逆性)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Octave-Tutorial"><span class="nav-number">5.</span> <span class="nav-text">Octave Tutorial</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Operations"><span class="nav-number">5.1.</span> <span class="nav-text">Basic Operations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Moving-Data-Around"><span class="nav-number">5.2.</span> <span class="nav-text">Moving Data Around</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Computing-on-Data"><span class="nav-number">5.3.</span> <span class="nav-text">Computing on Data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Plotting-Data"><span class="nav-number">5.4.</span> <span class="nav-text">Plotting Data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Control-Statements-for-while-if-statement"><span class="nav-number">5.5.</span> <span class="nav-text">Control Statements: for, while, if statement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Functions"><span class="nav-number">5.6.</span> <span class="nav-text">Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vectorization"><span class="nav-number">5.7.</span> <span class="nav-text">Vectorization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#作业"><span class="nav-number">5.8.</span> <span class="nav-text">作业</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logistic-Regression-逻辑回归"><span class="nav-number">6.</span> <span class="nav-text">Logistic Regression (逻辑回归)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Classification-and-Representation"><span class="nav-number">6.1.</span> <span class="nav-text">Classification and Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分类问题"><span class="nav-number">6.1.1.</span> <span class="nav-text">分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hypothesis-Representation"><span class="nav-number">6.1.2.</span> <span class="nav-text">Hypothesis Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decision-Boundary-决策边界"><span class="nav-number">6.1.3.</span> <span class="nav-text">Decision Boundary (决策边界)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression-Model"><span class="nav-number">6.2.</span> <span class="nav-text">Logistic Regression Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#成本函数"><span class="nav-number">6.2.1.</span> <span class="nav-text">成本函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简化成本函数和梯度下降"><span class="nav-number">6.2.2.</span> <span class="nav-text">简化成本函数和梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advanced-Optimization-高级优化"><span class="nav-number">6.2.3.</span> <span class="nav-text">Advanced Optimization (高级优化)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiclass-Classification"><span class="nav-number">6.3.</span> <span class="nav-text">Multiclass Classification</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Regularization"><span class="nav-number">7.</span> <span class="nav-text">Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Solving-the-Problem-of-Overfitting"><span class="nav-number">7.1.</span> <span class="nav-text">Solving the Problem of Overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Problem-of-Overfitting-过度拟合问题"><span class="nav-number">7.1.1.</span> <span class="nav-text">The Problem of Overfitting (过度拟合问题)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#成本函数-1"><span class="nav-number">7.1.2.</span> <span class="nav-text">成本函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularized-Linear-Regression-正则化线性回归"><span class="nav-number">7.1.3.</span> <span class="nav-text">Regularized Linear Regression (正则化线性回归)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularized-Logistic-Regression-正则化逻辑回归"><span class="nav-number">7.1.4.</span> <span class="nav-text">Regularized Logistic Regression (正则化逻辑回归)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#作业-1"><span class="nav-number">7.2.</span> <span class="nav-text">作业</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#先说ex2"><span class="nav-number">7.2.1.</span> <span class="nav-text">先说ex2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#接下来是ex2-reg"><span class="nav-number">7.2.2.</span> <span class="nav-text">接下来是ex2_reg</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Neural-Networks-Representation"><span class="nav-number">8.</span> <span class="nav-text">Neural Networks: Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivations"><span class="nav-number">8.1.</span> <span class="nav-text">Motivations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-linear-Hypotheses"><span class="nav-number">8.1.1.</span> <span class="nav-text">Non-linear Hypotheses</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neurons-and-the-Brain"><span class="nav-number">8.1.2.</span> <span class="nav-text">Neurons and the Brain</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Networks"><span class="nav-number">8.2.</span> <span class="nav-text">Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Representation-I"><span class="nav-number">8.2.1.</span> <span class="nav-text">Model Representation I</span></a></li></ol></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yibei Chen</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'yibeichen';
      var disqus_identifier = '2016/08/13/Machine-Learning/';
      var disqus_title = "Machine Learning 课程笔记 (未完)";
      var disqus_url = 'http://yibeichen.me/2016/08/13/Machine-Learning/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
